<!DOCTYPE html>
<html lang="en">
Last Updated: April 21, 2025
<div style="text-align:center">
<head>
	<title>Iris Lew - About Me</title>
	<link rel="stylesheet" type="text/css" href="style.css">
</head>

<body>
		<!-- ABOUT ME DESCRIPTION -->
	<h1>Iris Lew</h1>
	<!-- <h2>An insightful data scientist and AI researcher who delivers data-driven solutions</h2>  # DON'T INCLUDE THE TAGLINE-->
		<div style="display:inline-block; vertical-align:top;">
			<img class="my-picture" src="images/il.jpg">
		</div> <!-- end of picture alignment -->
		<div style="display:inline-block;text-align: left;">
			<br>
			<p>I am
			<ul>
				<li>a <b>critical thinker</b> who uses a data-driven approach to guide decision making</li>
				<li>a <b>translator</b> of complex technical information into best practices and documentation</li>
				<li>adept at <b>identifying a high-quality solution</b> that stays within the budget and meets client needs</li>
				<li>an <b>independent, innovative project manager</b> who consistently leads the team to deliver actionable insights before deadlines</li>
				<li>a <b>flexible creator</b> who can design and deploy production-ready advanced models</li>
			</ul>
			</p>
		</div> <!-- end of self-description -->
	
	<!-- THE SLIDER/TOGGLE -->
	<div class="container">
		<label class="toggle">
			<input type="checkbox">
			<span class="slider"></span>
			<span class="labels" data-on="CV" data-off="Projects" onclick="toggle_func()"></span>
		</label>
	<!-- MY PROJECTS -->
		<br>
		<div class="projects_grid" id="projects_grid_id">
			<div class="grid-item"><b>Title</b></div>
			<div class="grid-item"><b>Description</b></div>
			<div class="grid-item"><b>External Links</b></div>

			<div class="grid-item">Taskmaster Dashboard</div>
			<div class="grid-item">
				Taskmaster is a British panel show I started watching in 2020. Throughout each series of the show, five contestants will complete tasks
				and judged based by the Taskmaster, Greg Davies. Fans of the show have diligently compiled the number of the points awarded to each contestant, 
				the type of tasks completed, any mistakes made, and more. This gives me all sorts of numbers and statistics I could then visualize in a dashboard,
				including seeing if there are any key performance indicators (KPIs) that may impact their overall scores.
				<br><br>
				I sourced the <a href="https://taskmaster.info" target="_blank">Taskmaster wiki</a> for the points each contestant earned for which tasks and 
				created my own spreadsheets. I wanted the spreadsheets to be "raw data" and most of the data cleaning and calculations to be done with Tableau. Other people have 
				created their own spreadsheets, but I found that most of the spreadsheets don't include the calculations done for other series of the Taskmaster 
				franchise (e.g., Taskmaster NZ, Kongen Befaler); I wanted the calculations applied to all series, rather than just the original UK version. 
				I referenced Jack Bernhardt's spreadsheet for the calculations and to double-check my work. 
				<br><br>
				At the end of it all, I was able to engage with a TV series I enjoyed more because I was able to visualize the data rather than just 
				seeing them as numbers. The points are meaningless, but I was able to create radar graphs, bump charts, and bar graphs. I could use 
				parameters to filter out for the highest score of each category and which contestant achieved that score. 
				If you would like to interact with the published dashboard, you can click on "Tableau" over on the right.
				Unfortunately, when I publish to Tableau Public, not all the formatting carried over. 
				<br><br>
				<b><i>Next Steps: </i></b>Create worksheets that compare across series and across franchises.
				<br><br>
				<img id="myImg" src="./project_007_TaskmasterDashboard/series5.jpg" alt="Series 5" style="width:10%;max-width:1601px;border: 1px solid #555;">
				<!-- The Modal -->
				<div id="myModal" class="modal">
				<!-- The Close Button -->
				<span class="close">&times;</span>
				<!-- Modal Content (The Image) -->
				<img class="modal-content" id="Taskmaster">
				<!-- Modal Caption (Image Text) -->
				<div id="caption">Taskmaster Dashboard</div>
				</div>
				<br><br>
				<i>Software: Tableau</i>
				<br><br>
			</div> <!-- end of Taskmaster_dashboard description-->
			<div class="grid-item"><a href="https://public.tableau.com/views/Taskmaster_17394115892310/CurrentSeason?:language=en-US&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link" target="_blank">Tableau</a> </div>

			<div class="grid-item">Automatic Essay Scoring</div>
			<div class="grid-item">
				Automatic essay scoring (AES) is a machine learning problem where essays are parsed through 
				a machine learning model and a score is assigned. Since a score is assigned by a machine, it must 
				be unbiased...right? I explore the performance of a BERT-base-cased model that is fine-tuned on 
				student essays and the predicted scores of two different datasets. Furthermore, I test the performance 
				of the model and whether it will vary based on prompt.
				<br><br>
				I use two datasets: the ASAP-AES (train only) dataset and ELLIPSE corpus. The ASAP-AES dataset is 
				assumed to be composed of essays that are written by students in general,  but the ELLIPSE corpus 
				contains essays that are specifically written by English Language Learners (ELLs).
				<br><br>
				There are problems with using two different datasets. The first is that both datasets use different scores. As the 
				goal of this project is to see if there is any differences in the predicted scores, it should be sufficient to look at the 
				distribution when comparing. The second is that the ASAP-AES dataset doesn't have consistent scoring criteria amongst its own
				sets. 
				<br><br>
				I played around with the training and test data to see what happens when I add more and more ASAP-AES essay sets into the training data and using the other sets as a test set. 
				This is to mimic what would happen if the structure of an essay is the same, but the prompt changes from year to year. 
				The accuracy of the ASAP-AES test set increases with more essays, 
				but only if the scoring criteria doesn't change drastically. 
				Once the scoring criteria changes, especially with Sets 6 and 7, the model fails to predict any ASAP-AES scores. 
				When all the sets were mixed together and 10% was reserved for the test set, I have an accuracy of about 57.7%. 
				<br><br>
				Generally, the ELLIPSE Corpus received a higher predicted score than the ASAP-AES essays so it doesn't seem to be biased against ELLs. 
				Even when the ASAP-AES training data was varied, the predictions on the ELLIPSE 
				test data was generally the same, but the only model which has a mixture of all 8 sets as part of the training data (only 90% of the ASAP-AES set) was able to predict 
				scores higher than 20. This shows how much the model depends on the training data it is fine-tuned on and the failures of interpreting the scores outputted
				(i.e., the same essay can receive a score of 4 or 10 and it's not clear why it's different other than the training data).
				<br><br>
				While building this, I followed a guide and adapted most of the code from the guide for use in AES_2_model_no_custom.ipynb and AES_3_model_set_order.ipynb.
				<br><br>
				<i>Update (Feb 3, 2025):</i> I updated to include notebooks where I developed a smooth version of a quadratic weighted kappa loss function and 
				use it in the model. Unfortunately, the model is overfitting and memorizing the predictions.
				<br><br>
				<b><i>Next Steps: </i></b> Update the notebooks to use a quadratic weighted kappa as a metric rather than accuracy. Fix the smooth quadratic weighted kappa function.
				<br>
				<br>
			<i>
				Language: Python<br>
				Python Libraries: NumPy, Pandas, PyTorch, PIL, wordcloud, transformers, MatPlotLib, Seaborn, scikit-learn, transformers, time, datetime,
				SciPy<br>
				Architecture: PyTorch <br>
				Statistics: Large Language Model
			</i>
			<br>
			<br>
			</div> <!-- end of AES description-->
			<div class="grid-item"><a href="https://github.com/iris-lew/projects/tree/main/project_006_AES" target="_blank">GitHub</a> </div>
			
			<div class="grid-item">Identifying Bad Datasets</div>
			<div class="grid-item">
				Online forums recognize that they are a treasure trove for data mining as their data is used to train models, 
				and companies have taken to paywalling API access to them, most notably Reddit and X (formerly Twitter).
				So why is training data valuable? I dive into what happens to a model when it attempts to answer a 
				particular question where the training data isn't the best, but is publicly available. It is also a lesson 
				in critical thinking with data, and why we can't take promises at their face value.
				<br><br>
				I use two wildfire datasets taken from Kaggle in this write up. I find that using the first dataset 
				to create a neural net model isn't truly predicting wildfires despite its ~90% accuracy and 
				instead likely predicting urbanism. The second dataset is a wildfire dataset that contains a lot information 
				on the fires, where they occurred, how large they were, how long it burned for, and more. I recognized 
				that it was likely more useful than the first in predicting wildfires, but there was no specific "outline" for 
				how to use the information. 
				<br><br>
				I originally used these datasets in my graduate program 
				as part of a group project, and am reusing some of the code from that project in the write up. The chunks
				that are not from my code are denoted.
				<br>
				<br>
			<i>
				Language: Python<br>
				Python Libraries: NumPy, Pandas, MatPlotLib, PIL, folium, cv2, sk-learn, TensorFlow, random, pickle, os, json, 
				landsatxplore, tifffile
				Statistics: deep learning in computer vision
			</i>
			<br>
			<br>
			</div> <!-- end of bad_dataset description-->
			<div class="grid-item"><a href="https://github.com/iris-lew/projects/tree/main/project_005_IdentifyingBadData" target="_blank">GitHub</a></div>		

			<div class="grid-item">Do English Language Learners Write Like Published Authors?</div>
			<div class="grid-item">
				For our DATASCI W266 (Natural Language Processing with Deep Learning) class,
				Srila and I examined whether the performance of BERT models could be improved in scoring 
				essays written by English Language Learners (ELLs).
				Prior studies suggest that the pre-trained data (Google BooksCorpus and Wikipedia) 
				of BERT was unsuited for automatic essay scoring,
				thus we decided to experiment with unfreezing the pretrained layers of BERT-base-cased and BERTweet-base.
				We decided to also experiment with BERTweet because it is pretrained on Tweets,
				and we are hoping that its informal nature will be more reminiscent of student writing.
				<br><br>
				We find that depending less on the pretraining weights (with more layers unfrozen) provided more accurate predictions 
				but BERT-base-cased outperformed BERTweet-base suggesting that student essays belonged to a different population than Tweets.
				Together, the results indicate that ELL student essays don't contain writing similar to published authors (Google BooksCorpus), 
				Wikipedia editors, or Tweets; therefore, models that don't contain training specific to student essays 
				are unlikely to produce scores that reflect their actual grade.
				<br><br>
				Srila wrote the majority of the code while I did the background research, piecing the code together,
				and the writing.
				<br><br>
				<i>Language: Python<br>
				Python Libraries: MatPlotLib, Seaborn, NumPy, Pandas, transformers, sklearn, nltk, wordcloud, SciPy, yellowbrick, Torch<br>
				Architecture: TensorFlow and Keras<br>
				Statistics: deep learning in large language models</i>
				<br>
				<br>
			</div> <!-- end of ELL description-->
			<div class="grid-item"><a href="https://github.com/iris-lew/projects/tree/main/project_004_DoELLStudentsWriteLikePublishedAuthors" target="_blank">GitHub</a></div>

			<div class="grid-item">Phishing for Significance</div>
			<div class="grid-item">
				For our DATASCI W241 (Experiments and Causal Inference) class, 
				my group and I were concerned about whether cybersecurity training could be improved so that less students would 
				be victims of online recruitment fraud.
				<br><br>
				At the time this project occurred, there was no cybersecurity training that was provided to the students, 
				and the university only offered "Fight the Phish" posters which we had to search for ourselves. 
				We created a multi-factorial experiment where we used a survey to test whether having interactive cybersecurity training, as compared to training where 
				which only requires reading, would increase the accuracy rate of identification rate of phishing emails that are targeted to students.
				In order to ensure that the respondents weren't just clicking through the training, we also included a timer so that the respondents 
				had to stop and read through the training. For the measurement of the results, there were 10 emails, 
				5 of which are real recruitment emails sent to the three of us but with the names changed and 5 of which are fake emails I wrote.
				<br><br>
				We find that there is no statistically significant difference in the identification rates between the students who 
				received the interactive training vs. the passive training. The responses indicated that the emails looked "phishy"
				in the first place, but a chi-square test revealed that there's no statistically significant difference between identifying
				emails as phishing and whether they were actual phishing emails in the first place. The timer did not improve the respondent's 
				accuracy rates of identifying phishing emails either.
				<br><br>
				While this was originally done in conjunction with Angelique Agho and Srila Maiti,
				most of the Qualtrics programming, R code, and writing is mine.
				<br><br>
				<i>Qualtrics link: <a href="https://berkeley.qualtrics.com/jfe/form/SV_0HZ8Urm7Wng0zzg">
					https://berkeley.qualtrics.com/jfe/form/SV_0HZ8Urm7Wng0zzg</a><br>
				Language: R<br>
				R Libraries: data.table, lmtest, sandwich, reshape2, stargazer, diagram, tidyverse, ggpubr<br>
				Statistics: Summary statistics, linear regression, A/B testing
				</i>
				<br>
				<br>
			</div> <!-- end of phishing description-->
			<div class="grid-item"><a href="https://github.com/iris-lew/projects/tree/main/project_002_PhishingForSignificance" target="_blank">GitHub</a></div>
			
			<div class="grid-item">LA Traffic Collisions</div>
			<div class="grid-item">
				Vision Zero's goal is to eliminate traffic fatalities and severe injuries. Los Angeles (LA) is infamous for its traffic and high speed highway chases, 
				and it adopted Vision Zero in 2015 in an attempt to reduce the
				traffic deaths to 0. There is inconsistent messaging on whether it is aimed at LA as a whole or only on unincorporated county roadways.
				The analysis is based on Vision Zero's impact on LA as a whole. 
				<br><br>
				Using LA City's Traffic Collision data, I did some exploratory data analysis and regression discontinuity analysis on the number of traffic collisions, the number of victims, and the number of victims
				with severe or fatal injuries. For the most part, the collisions increased from 2015 onwards until COVID-19 was declared a pandemic.
				Most of the collisions were in streets (95.4%) and parking lots (3.2%). There are certain intersections 
				(e.g., the intersection of Sepulveda Blvd and Sherman Way) where there are more collisions. 
				Most collisions were vehicle vs. vehicle collisions, but not all them resulted in a severe or fatal injury.
				Out of 370,447 collisions which has a code associated with injury, only 13,794 (~3.7%) had a report of a severe or fatal injury.
				The regression discontinuity analysis determined that there was a statistically significant reduction in the number of traffic collisions, 
				likely due to the  COVID-19 pandemic, but not due to Vision Zero. In fact, after Vision Zero was signed, 
				there was an increase in the number of collisions. However, there seemed to be little to no difference in the number of 
				collisions with severe or fatal injuries regardless of whether Vision Zero was signed or Covid-19 declared as a pandemic. 
				<br><br>
				While this was originally a group project by Ben Meier, Rebecca Sun, and me for our DATASCI W200 class together where we only did data exploration, I redid the writing and the code to be mine.
				<br><br>
				<i>Update (Nov 10, 2024):</i> I updated this analysis to include a regression discontinuity design.
				The regression discontinuity design evaluates the effect of Vision Zero as well as the Covid-19 pandemic
				on the number of collisions, the number of victims, and the number of collisions where there were severe or fatal injuries
				recorded.
				<br><br>
				<i>Update (Feb 10, 2025):</i> I re-ran the analysis on only the unincorporated areas of LA. There was no statistical significance 
				from the regression discontinuity test where the cutoff is when Vision Zero was signed. There was statistical significance due to 
				the declaration of Covid-19 as a pandemic. When analyzing only the collisions where there was a severe or fatal injuries, I 
				was not able to run an analysis but visually, there was no change (it was only 1 collision a day throughout). Vision Zero 
				appears to be ineffective in the unincorporated areas. If it were effective, it should be enacted on the incorporated areas 
				because that is where more collisions occur and contains more collisions with fatal or severe injuries.
				<br><br>
				<b><i>Next steps:</i></b> Figure out what the other regression discontinuity graphs from the example does and mean.
				<br><br>
				<i>Language: Python<br>
				Python Libraries: MatPlotLib, Seaborn, NumPy, Pandas, Folium, rdrobust, copy, GeoPandas (GIS), shapely<br>
				Statistics: Summary statistics, Regression Discontinuity with robust standard errors</i>
				<br>
				<br>
			</div> <!-- end of LA_traffic description -->
			<div class="grid-item"><a href="https://github.com/iris-lew/projects/tree/main/project_001_LATrafficCollisions" target="_blank">GitHub</a></div>
			
			<div class="grid-item">LinkedIn Recommendations in a Word Cloud</div>
			<div class="grid-item">
				I've previously used Word Clouds through my work, so I was interested in exploring 
				how they functioned for myself through R and Tableau. 
				Additionally, I wanted to see if there were any common words that 
				people used to describe in their recommendations on LinkedIn so I combined these two desires.
				<br><br>
				The coding and write up was done by me.
				<br>
				<br>
			<i>
				Language: R<br>
				R Libraries: readxl, writexl, tm, tau, tidyr, RColorBrewer, wordcloud, wordcloud2,
				tidyverse, reshape2<br>
				Software: Tableau<br>
				Statistics: Bag of Words
			</i>
			<br>
			<br>
			</div> <!-- end of wordcloud description-->
			<div class="grid-item"><a href="https://github.com/iris-lew/projects/tree/main/project_003_RecommendationsWordcloud" target="_blank">GitHub</a></div>
		</div> <!-- end of project grid-->
		<br>
		<br>
	
		<!-- CV -->
		<div class="cv_describe" id="cv_describe_id">
			<h2>Experience</h2>
				<p2><b>University of California, Berkeley</b><br>
					<i>Reader (Teaching Assistant) | Jan 2, 2023 - Dec 15, 2023</i></p>
				<ul>
					<li>Instructed 70 graduate students in the UC Berkeley's School of Information and Data Science through office hours. 
						Used process-based and human-centric learning approaches to guide students through rigorous statistics problem sets in order to facilitate learners' 
						successful completion of upper division course work. </li>
					<li>Ensured client-centered education by collaborating with faculty members and improved curriculum and performance metrics to match students' learning capacities.</li>
					<li>Identified multiple incidences of academic plagiarism and took procedural actions in order to uphold compliance with UC Berkeley's Academic Code of Conduct Policy.</li>
					<li>Standardized survey experiments by forming a business connection between survey panels and course instructors. 
						Wrote a technical guide for students to follow so students are able to manage their own experiments independently. 
						Students were able to use advanced statistical analyses to create a predictive model for their own research question.</li>
					<li>Organized 9 presentations to showcase past research experiments to inspire current students to develop their own experiments. 
						Students were able to takeaway what made experiments more likely to show an effect, 
						plan their project so it would be completed within the given time frame, and to leverage subject matter experts.</li>
				</ul>
				<br>
				<p2><b>Morning Consult</b><br>
					<i>Data Scientist | Jun 8, 2021 - Jan 19, 2022</i></p>
				<ul>
					<li>Developed an R function to automatically compare and upload translations, 
						therefore increasing the efficiency of survey translations, 
						while ensuring accuracy by preventing cases where an update to the survey was made but the translation was forgotten.</li>
					<li>Routinely programmed and analyzed complex surveys involving stratisfied sampling and A/B tests containing up to 27 splits. 
						Results were used to make business decisions and some were featured in news reports.</li>
					<li>Developed a training guide for programming, fielding, and analyzing surveys that served as the foundation for all subsequent new hire trainings.</li>
				</ul>
				<br>
				<p2><b>Center for the Study of Services</b><br>
					<i>Research Associate | Feb 4, 2019 - May 3, 2021</i></p>
				<ul>
					<li>Served as a project lead and secondary project lead of two projects where I communicated with clients, 
						ensured fulfillment of contracts and compliance with state reporting requirements.</li>
					<li>Through leveraging data from the American Community Survey (ACS), I was able to use SQL to identify potential respondents 
						who would require the services of a bilingual phone interviewer, therefore raising the Spanish respondent rates by approximately 2 percentage points.</li>
					<li>By looking at historical trends regarding phone interviews, I was able to save the client over $10,000 by suggesting they switch protocols while achieving a similar result.</li>
					<li>Received data from three subcontractors and provided daily feedback to improve data quality over the course of six weeks, catching over 20 cases of incorrect data entry.</li>
					<li>I was able to decrease expenses of a specific invoice type by approximately 15% due to identifying a data error in less than 5 minutes using Microsoft Excel.</li>
					<li>Communicated with NCQA oversight to ensure equitable translations for Chinese survey respondents 
						as well as to meet compliance standards for email campaigns that boosted response rates by 9,400 responses (31% email response rate).</li>
					<li>Created and maintained two data pipelines which allowed for efficient standardized data cleaning and quality assurance.</li>
					<li>Wrote two technical documents for internal enterprise programs and another two guides detailing project workflow that helped new users 
						and freed up time for programmers to work on other projects.</li>
					<li>Created two client-facing Tableau dashboards and produced 28 reports using in-house automation software that received client praise. 
						Dashboards contained reporting rates and a mid-fielding report so clients and stakeholders were able to access key performance indicators (KPIs) 
						at their own convenience. Recommendations based on KPIs were provided in document reports.</li>
					<li>Supported company diversity initiatives through working groups.</li>
				</ul>
				<br>
				<p2><b>Hanover Research</b><br>
					<i>Senior Research Associate | Oct 2, 2017 - Feb 1, 2019</i></p>
				<ul>
					<li>Received a promotion in less than a year (Research Associate to Senior Research Associate).</li>
					<li>Received the Above and Beyond Award on three separate occasions within one year.</li>
					<li>"Ensuring Access to Programming for All Students: An Evaluation of Equity in a District's International 
						Baccalaureate Programming." AERA 2019. Leila Richey Nuland, Hanover Research; Michael Greenfield, 
						Harrison Central School District; Cate Keller, Hanover Research; Nadya Abramava, Hanover Research; Joy Gitter; 
						Iris Lew</li>
					<li>Showcased a Tableau dashboard and report at a company All-Stars firm-wide meeting.</li>
					<li>Voluntarily designed and led an R tutorial training as well as hosting R Learning Lunches for fellow teammates with 
						positive feedback from the entire team. 
						This fulfilled a gap in helping the team transition to using R which aligned with leadership goals.</li>
					<li>Designed surveys, analyzed the data, and wrote the report to clients and their stakeholders. 
						On one occasion, I programmatically cleaned and merged data from over 200 spreadsheets within one week, 
						saving 2 weeks of time while ensuring the process was replicable in future years. This demonstrated 
						to the team that R was able to do the exact same processes the team was doing in Excel, but 
						the automation could save hours of effort.</li>
				</ul>
				<br>
				<p2><b>Student Researcher at UC Berkeley</b><br>
					<i>May 2017 - Sept 2017</i></p>
				<ul>
					<li>Bangladesh Project: Performed secondary research to analyze how aid-driven industries can lead to health 
						and environmental crises in Bangladesh</li>
					<li>Power Africa Project: Performed secondary research to analyze the construction of the Gibe III Dam 
						and its relationship to Power Africa</li>
				</ul>
				<br>
				<p2><b>Top Notch Energy</b><br>
					<i>Secretary | May 2012 - Sept 2014</i></p>
				<ul>
					<li>Helped migrate the company from a pen and paper system to an electronic system</li>
					<li>Was a friendly, amicable, and professional secretary who welcomed customers and answered phone calls</li>
				</ul>
			<h2>Volunteer</h2>
				<p2><b>Cal Hacks</b><br>
					<i>Event Volunteer | Oct 2023 - Present</i></p>
				<ul>
					<li>Participate as member of larger hacker community and network of Bay Area technology professionals and contribute to Cal Hacks 10.0 and AI Hackathon 2.0 
						by directing and managing crowds of over 100+ ensuring event safety, efficiency, and success. </li>
				</ul>
				<br>
				<p2><b>The Contra Costa Asian American Pacific Islander Advocacy Coalition</b><br>
					<i>Community Health Worker and Advocate | Jan 2024 - Present</i></p2>
				<ul>
					<li>Acting as Coalition Community Health Worker and Advocate, 
						partner with Contra Costa County Health Services to provide vital health education and 
						outreach to low-income, priority populations in response to recent Tuberculosis outbreak in Richmond, CA.</li>
					<li>Collaborate with community partners to initiate process of assessing, registering, 
						and enrolling low-income Asian immigrant community members in Medi-Cal/Medicaid health coverage; 
						carry out Coalition's Multi-Cultural Community Health Worker Alliance Initiative; 
						educate public and promote Community Health Worker Benefits Addition under the Cal-AIM Medi-Cal Reform.</li>
				</ul>
			<h2>Coaching</h2>
			<p2><b>Introduction to Tableau | Hanover Research</b><br>
				I designed a Tableau training and piloted a demonstration of the training to both survey 
				and secondary research staff members. 
				This training focused on teaching staff members to create various different graphs (e.g., bar graphs, likert scales, 
				NPS scales), 
				to understand how Tableau reads the data, and how to manipulate data to get it in the desired format for Tableau.</p2>
			<br>
			<br>
			<p2><b>Introduction to R | Hanover Research</b><br>
				I designed and administered an R training that was targeted to survey researchers and I 
				later adapted this to fit secondary researchers too. 
				This training encompassed the topics of set up, vocabulary, if-statements, for loops, and while loops.
			</p2>
			<br>
			<br>
			<p2><b>Association of Psychology Undergraduates | UC Berkeley</b><br>
				I served as a mentor for two years to one mentee. 
				I recommended courses, provided textbooks, and answered any and all questions she may have 
				had regardless of topic.<br>
				<blockquote>
				"If possible, I would give her 100000. 
				Iris is an amazing mentor. 
				She is so smart and successful yet she is so down to earth, funny, and just straight out friendly! 
				She is very knowledgeable about everything that you need to know about psych major, 
				research, study abroad, random classes, etc. She is just simply amazing." <br>
				-Mentee
				</blockquote>
			</p2>
			<br>

			<h2>Education</h2>
			<p2><b>University of California - Berkeley | School of Information</b><br>
				<i>Master in Information and Data Science | Jan 2022 - Dec 2023</i></p>
				<ul>
					<li>Designed and presented projects using data science and machine learning</li>
					<li>Co-hosted a Course Selection Office Hour as a Student Representative</li>
					<li>Courses Completed</li>
					<ul>
						<li>DATASCI W200: Introduction to Data Science Programming</li>
						<li>DATASCI W201: Research Design and Applications for Data and Analysis</li>
						<li>DATASCI W203: Statistics for Data Science</li>
						<li>DATASCI W205: Fundamentals of Data Engineering</li>
						<li>DATASCI W207: Applied Machine Learning</li>
						<li>DATASCI W241: Experiments and Causal Inference</li>
						<li>DATASCI W266: Natural Language Processing with Deep Learning </li>
						<li>DATASCI W255: Machine Learning Systems Engineering </li>
						<li>DATASCI W210: Capstone </li>
					</ul>
				</ul> 
				<br>
			<p2><b>Northern Virginia Community College</b><br>
			<i>Professional Development | Jun 2020 - Aug 2020</i></p>
			<ul>
				<li>Courses Completed</li>
				<ul>
					<li>MTH 266: Linear Algebra</li>
				</ul>
			</ul>
			<br>
			<p2><b>University of Cambridge | Pembroke College</b><br>
				<i>Study Abroad, Psychology | Jun 2016 - Aug 2016</i></p>
				<ul>
					<li>Wrote 5 research papers on the following topics: 
						heuristics, Prospect Theory, the Somatic Marker Hypothesis, 
						clinical conditions affecting decision making, and reasoning</li>
					<li>Courses Completed</li>
					<ul>
						<li>Supervision (Independent Research)</li>
						<li>Psych of Language</li>
						<li>Spooks and Spies</li>
					</ul>
				</ul>
				<br>
			<p2><b>University of California - Berkeley | College of Letters and Science</b><br>
				<i>Bachelor of Arts in Psychology and Minor in Conservation and Resource Studies | Aug 2014 - May 2017</i></p>
				<ul>
					<li>Graduated with Honors and Distinction in General Scholarship</li>
					<li>Honors Societies: Psi Chi (Psychology) and Phi Beta Kappa</li>
					<li>Honors Thesis: "Examining a Violation of Procedure Invariance Using Willingness to Pay and Choice" 
						<ul>
							<li>Conducted 4 online laboratory experiments using Qualtrics and Amazon mTurk</li>
							<li>Did the literature review, implemented the experiments, analyzed and coded the results, wrote the manuscript</li>
						</ul>
					</li>
					<li>Relationships and Social Cognition Lab - SES, Stress, and Emotion Regulation Project (2016-2017):
						<ul>
							<li>Used R to analyze results</li>
							<li>Manually coded up results in Excel</li>
							<li>Learned how to program experiments using Inquisit</li>
						</ul>
					</li>
				</ul>
		</div> <!-- end of CV -->
	</div> <!-- end of container for everything below slider toggle-->
</div> <!-- end of everything being centered -->

</body>
<script src="main.js"></script>
</html>